{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# import optuna\n",
    "# from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "import tensorboard\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED              = 42 # 69420\n",
    "TRAIN_COLUMNS     = [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\", \"g-0\", \"g-1\", \"g-2\", \"g-3\", \"g-4\", \"g-5\", \"g-6\", \"g-7\", \"g-8\", \"g-9\", \"g-10\", \"g-11\", \"g-12\", \"g-13\", \"g-14\", \"g-15\", \"g-16\", \"g-17\", \"g-18\", \"g-19\", \"g-20\", \"g-21\", \"g-22\", \"g-23\", \"g-24\", \"g-25\", \"g-26\", \"g-27\", \"g-28\", \"g-29\", \"g-30\", \"g-31\", \"g-32\", \"g-33\", \"g-34\", \"g-35\", \"g-36\", \"g-37\", \"g-38\", \"g-39\", \"g-40\", \"g-41\", \"g-42\", \"g-43\", \"g-44\", \"g-45\", \"g-46\", \"g-47\", \"g-48\", \"g-49\", \"g-50\", \"g-51\", \"g-52\", \"g-53\", \"g-54\", \"g-55\", \"g-56\", \"g-57\", \"g-58\", \"g-59\", \"g-60\", \"g-61\", \"g-62\", \"g-63\", \"g-64\", \"g-65\", \"g-66\", \"g-67\", \"g-68\", \"g-69\", \"g-70\", \"g-71\", \"g-72\", \"g-73\", \"g-74\", \"g-75\", \"g-76\", \"g-77\", \"g-78\", \"g-79\", \"g-80\", \"g-81\", \"g-82\", \"g-83\", \"g-84\", \"g-85\", \"g-86\", \"g-87\", \"g-88\", \"g-89\", \"g-90\", \"g-91\", \"g-92\", \"g-93\", \"g-94\", \"g-95\", \"g-96\", \"g-97\", \"g-98\", \"g-99\", \"g-100\", \"g-101\", \"g-102\", \"g-103\", \"g-104\", \"g-105\", \"g-106\", \"g-107\", \"g-108\", \"g-109\", \"g-110\", \"g-111\", \"g-112\", \"g-113\", \"g-114\", \"g-115\", \"g-116\", \"g-117\", \"g-118\", \"g-119\", \"g-120\", \"g-121\", \"g-122\", \"g-123\", \"g-124\", \"g-125\", \"g-126\", \"g-127\", \"g-128\", \"g-129\", \"g-130\", \"g-131\", \"g-132\", \"g-133\", \"g-134\", \"g-135\", \"g-136\", \"g-137\", \"g-138\", \"g-139\", \"g-140\", \"g-141\", \"g-142\", \"g-143\", \"g-144\", \"g-145\", \"g-146\", \"g-147\", \"g-148\", \"g-149\", \"g-150\", \"g-151\", \"g-152\", \"g-153\", \"g-154\", \"g-155\", \"g-156\", \"g-157\", \"g-158\", \"g-159\", \"g-160\", \"g-161\", \"g-162\", \"g-163\", \"g-164\", \"g-165\", \"g-166\", \"g-167\", \"g-168\", \"g-169\", \"g-170\", \"g-171\", \"g-172\", \"g-173\", \"g-174\", \"g-175\", \"g-176\", \"g-177\", \"g-178\", \"g-179\", \"g-180\", \"g-181\", \"g-182\", \"g-183\", \"g-184\", \"g-185\", \"g-186\", \"g-187\", \"g-188\", \"g-189\", \"g-190\", \"g-191\", \"g-192\", \"g-193\", \"g-194\", \"g-195\", \"g-196\", \"g-197\", \"g-198\", \"g-199\", \"g-200\", \"g-201\", \"g-202\", \"g-203\", \"g-204\", \"g-205\", \"g-206\", \"g-207\", \"g-208\", \"g-209\", \"g-210\", \"g-211\", \"g-212\", \"g-213\", \"g-214\", \"g-215\", \"g-216\", \"g-217\", \"g-218\", \"g-219\", \"g-220\", \"g-221\", \"g-222\", \"g-223\", \"g-224\", \"g-225\", \"g-226\", \"g-227\", \"g-228\", \"g-229\", \"g-230\", \"g-231\", \"g-232\", \"g-233\", \"g-234\", \"g-235\", \"g-236\", \"g-237\", \"g-238\", \"g-239\", \"g-240\", \"g-241\", \"g-242\", \"g-243\", \"g-244\", \"g-245\", \"g-246\", \"g-247\", \"g-248\", \"g-249\", \"g-250\", \"g-251\", \"g-252\", \"g-253\", \"g-254\", \"g-255\", \"g-256\", \"g-257\", \"g-258\", \"g-259\", \"g-260\", \"g-261\", \"g-262\", \"g-263\", \"g-264\", \"g-265\", \"g-266\", \"g-267\", \"g-268\", \"g-269\", \"g-270\", \"g-271\", \"g-272\", \"g-273\", \"g-274\", \"g-275\", \"g-276\", \"g-277\", \"g-278\", \"g-279\", \"g-280\", \"g-281\", \"g-282\", \"g-283\", \"g-284\", \"g-285\", \"g-286\", \"g-287\", \"g-288\", \"g-289\", \"g-290\", \"g-291\", \"g-292\", \"g-293\", \"g-294\", \"g-295\", \"g-296\", \"g-297\", \"g-298\", \"g-299\", \"g-300\", \"g-301\", \"g-302\", \"g-303\", \"g-304\", \"g-305\", \"g-306\", \"g-307\", \"g-308\", \"g-309\", \"g-310\", \"g-311\", \"g-312\", \"g-313\", \"g-314\", \"g-315\", \"g-316\", \"g-317\", \"g-318\", \"g-319\", \"g-320\", \"g-321\", \"g-322\", \"g-323\", \"g-324\", \"g-325\", \"g-326\", \"g-327\", \"g-328\", \"g-329\", \"g-330\", \"g-331\", \"g-332\", \"g-333\", \"g-334\", \"g-335\", \"g-336\", \"g-337\", \"g-338\", \"g-339\", \"g-340\", \"g-341\", \"g-342\", \"g-343\", \"g-344\", \"g-345\", \"g-346\", \"g-347\", \"g-348\", \"g-349\", \"g-350\", \"g-351\", \"g-352\", \"g-353\", \"g-354\", \"g-355\", \"g-356\", \"g-357\", \"g-358\", \"g-359\", \"g-360\", \"g-361\", \"g-362\", \"g-363\", \"g-364\", \"g-365\", \"g-366\", \"g-367\", \"g-368\", \"g-369\", \"g-370\", \"g-371\", \"g-372\", \"g-373\", \"g-374\", \"g-375\", \"g-376\", \"g-377\", \"g-378\", \"g-379\", \"g-380\", \"g-381\", \"g-382\", \"g-383\", \"g-384\", \"g-385\", \"g-386\", \"g-387\", \"g-388\", \"g-389\", \"g-390\", \"g-391\", \"g-392\", \"g-393\", \"g-394\", \"g-395\", \"g-396\", \"g-397\", \"g-398\", \"g-399\", \"g-400\", \"g-401\", \"g-402\", \"g-403\", \"g-404\", \"g-405\", \"g-406\", \"g-407\", \"g-408\", \"g-409\", \"g-410\", \"g-411\", \"g-412\", \"g-413\", \"g-414\", \"g-415\", \"g-416\", \"g-417\", \"g-418\", \"g-419\", \"g-420\", \"g-421\", \"g-422\", \"g-423\", \"g-424\", \"g-425\", \"g-426\", \"g-427\", \"g-428\", \"g-429\", \"g-430\", \"g-431\", \"g-432\", \"g-433\", \"g-434\", \"g-435\", \"g-436\", \"g-437\", \"g-438\", \"g-439\", \"g-440\", \"g-441\", \"g-442\", \"g-443\", \"g-444\", \"g-445\", \"g-446\", \"g-447\", \"g-448\", \"g-449\", \"g-450\", \"g-451\", \"g-452\", \"g-453\", \"g-454\", \"g-455\", \"g-456\", \"g-457\", \"g-458\", \"g-459\", \"g-460\", \"g-461\", \"g-462\", \"g-463\", \"g-464\", \"g-465\", \"g-466\", \"g-467\", \"g-468\", \"g-469\", \"g-470\", \"g-471\", \"g-472\", \"g-473\", \"g-474\", \"g-475\", \"g-476\", \"g-477\", \"g-478\", \"g-479\", \"g-480\", \"g-481\", \"g-482\", \"g-483\", \"g-484\", \"g-485\", \"g-486\", \"g-487\", \"g-488\", \"g-489\", \"g-490\", \"g-491\", \"g-492\", \"g-493\", \"g-494\", \"g-495\", \"g-496\", \"g-497\", \"g-498\", \"g-499\", \"g-500\", \"g-501\", \"g-502\", \"g-503\", \"g-504\", \"g-505\", \"g-506\", \"g-507\", \"g-508\", \"g-509\", \"g-510\", \"g-511\", \"g-512\", \"g-513\", \"g-514\", \"g-515\", \"g-516\", \"g-517\", \"g-518\", \"g-519\", \"g-520\", \"g-521\", \"g-522\", \"g-523\", \"g-524\", \"g-525\", \"g-526\", \"g-527\", \"g-528\", \"g-529\", \"g-530\", \"g-531\", \"g-532\", \"g-533\", \"g-534\", \"g-535\", \"g-536\", \"g-537\", \"g-538\", \"g-539\", \"g-540\", \"g-541\", \"g-542\", \"g-543\", \"g-544\", \"g-545\", \"g-546\", \"g-547\", \"g-548\", \"g-549\", \"g-550\", \"g-551\", \"g-552\", \"g-553\", \"g-554\", \"g-555\", \"g-556\", \"g-557\", \"g-558\", \"g-559\", \"g-560\", \"g-561\", \"g-562\", \"g-563\", \"g-564\", \"g-565\", \"g-566\", \"g-567\", \"g-568\", \"g-569\", \"g-570\", \"g-571\", \"g-572\", \"g-573\", \"g-574\", \"g-575\", \"g-576\", \"g-577\", \"g-578\", \"g-579\", \"g-580\", \"g-581\", \"g-582\", \"g-583\", \"g-584\", \"g-585\", \"g-586\", \"g-587\", \"g-588\", \"g-589\", \"g-590\", \"g-591\", \"g-592\", \"g-593\", \"g-594\", \"g-595\", \"g-596\", \"g-597\", \"g-598\", \"g-599\", \"g-600\", \"g-601\", \"g-602\", \"g-603\", \"g-604\", \"g-605\", \"g-606\", \"g-607\", \"g-608\", \"g-609\", \"g-610\", \"g-611\", \"g-612\", \"g-613\", \"g-614\", \"g-615\", \"g-616\", \"g-617\", \"g-618\", \"g-619\", \"g-620\", \"g-621\", \"g-622\", \"g-623\", \"g-624\", \"g-625\", \"g-626\", \"g-627\", \"g-628\", \"g-629\", \"g-630\", \"g-631\", \"g-632\", \"g-633\", \"g-634\", \"g-635\", \"g-636\", \"g-637\", \"g-638\", \"g-639\", \"g-640\", \"g-641\", \"g-642\", \"g-643\", \"g-644\", \"g-645\", \"g-646\", \"g-647\", \"g-648\", \"g-649\", \"g-650\", \"g-651\", \"g-652\", \"g-653\", \"g-654\", \"g-655\", \"g-656\", \"g-657\", \"g-658\", \"g-659\", \"g-660\", \"g-661\", \"g-662\", \"g-663\", \"g-664\", \"g-665\", \"g-666\", \"g-667\", \"g-668\", \"g-669\", \"g-670\", \"g-671\", \"g-672\", \"g-673\", \"g-674\", \"g-675\", \"g-676\", \"g-677\", \"g-678\", \"g-679\", \"g-680\", \"g-681\", \"g-682\", \"g-683\", \"g-684\", \"g-685\", \"g-686\", \"g-687\", \"g-688\", \"g-689\", \"g-690\", \"g-691\", \"g-692\", \"g-693\", \"g-694\", \"g-695\", \"g-696\", \"g-697\", \"g-698\", \"g-699\", \"g-700\", \"g-701\", \"g-702\", \"g-703\", \"g-704\", \"g-705\", \"g-706\", \"g-707\", \"g-708\", \"g-709\", \"g-710\", \"g-711\", \"g-712\", \"g-713\", \"g-714\", \"g-715\", \"g-716\", \"g-717\", \"g-718\", \"g-719\", \"g-720\", \"g-721\", \"g-722\", \"g-723\", \"g-724\", \"g-725\", \"g-726\", \"g-727\", \"g-728\", \"g-729\", \"g-730\", \"g-731\", \"g-732\", \"g-733\", \"g-734\", \"g-735\", \"g-736\", \"g-737\", \"g-738\", \"g-739\", \"g-740\", \"g-741\", \"g-742\", \"g-743\", \"g-744\", \"g-745\", \"g-746\", \"g-747\", \"g-748\", \"g-749\", \"g-750\", \"g-751\", \"g-752\", \"g-753\", \"g-754\", \"g-755\", \"g-756\", \"g-757\", \"g-758\", \"g-759\", \"g-760\", \"g-761\", \"g-762\", \"g-763\", \"g-764\", \"g-765\", \"g-766\", \"g-767\", \"g-768\", \"g-769\", \"g-770\", \"g-771\", \"c-0\", \"c-1\", \"c-2\", \"c-3\", \"c-4\", \"c-5\", \"c-6\", \"c-7\", \"c-8\", \"c-9\", \"c-10\", \"c-11\", \"c-12\", \"c-13\", \"c-14\", \"c-15\", \"c-16\", \"c-17\", \"c-18\", \"c-19\", \"c-20\", \"c-21\", \"c-22\", \"c-23\", \"c-24\", \"c-25\", \"c-26\", \"c-27\", \"c-28\", \"c-29\", \"c-30\", \"c-31\", \"c-32\", \"c-33\", \"c-34\", \"c-35\", \"c-36\", \"c-37\", \"c-38\", \"c-39\", \"c-40\", \"c-41\", \"c-42\", \"c-43\", \"c-44\", \"c-45\", \"c-46\", \"c-47\", \"c-48\", \"c-49\", \"c-50\", \"c-51\", \"c-52\", \"c-53\", \"c-54\", \"c-55\", \"c-56\", \"c-57\", \"c-58\", \"c-59\", \"c-60\", \"c-61\", \"c-62\", \"c-63\", \"c-64\", \"c-65\", \"c-66\", \"c-67\", \"c-68\", \"c-69\", \"c-70\", \"c-71\", \"c-72\", \"c-73\", \"c-74\", \"c-75\", \"c-76\", \"c-77\", \"c-78\", \"c-79\", \"c-80\", \"c-81\", \"c-82\", \"c-83\", \"c-84\", \"c-85\", \"c-86\", \"c-87\", \"c-88\", \"c-89\", \"c-90\", \"c-91\", \"c-92\", \"c-93\", \"c-94\", \"c-95\", \"c-96\", \"c-97\", \"c-98\", \"c-99\"]\n",
    "TARGET_COLUMNS    = ['5-alpha_reductase_inhibitor', '11-beta-hsd1_inhibitor', 'acat_inhibitor', 'acetylcholine_receptor_agonist', 'acetylcholine_receptor_antagonist', 'acetylcholinesterase_inhibitor', 'adenosine_receptor_agonist', 'adenosine_receptor_antagonist', 'adenylyl_cyclase_activator', 'adrenergic_receptor_agonist', 'adrenergic_receptor_antagonist', 'akt_inhibitor', 'aldehyde_dehydrogenase_inhibitor', 'alk_inhibitor', 'ampk_activator', 'analgesic', 'androgen_receptor_agonist', 'androgen_receptor_antagonist', 'anesthetic_-_local', 'angiogenesis_inhibitor', 'angiotensin_receptor_antagonist', 'anti-inflammatory', 'antiarrhythmic', 'antibiotic', 'anticonvulsant', 'antifungal', 'antihistamine', 'antimalarial', 'antioxidant', 'antiprotozoal', 'antiviral', 'apoptosis_stimulant', 'aromatase_inhibitor', 'atm_kinase_inhibitor', 'atp-sensitive_potassium_channel_antagonist', 'atp_synthase_inhibitor', 'atpase_inhibitor', 'atr_kinase_inhibitor', 'aurora_kinase_inhibitor', 'autotaxin_inhibitor', 'bacterial_30s_ribosomal_subunit_inhibitor', 'bacterial_50s_ribosomal_subunit_inhibitor', 'bacterial_antifolate', 'bacterial_cell_wall_synthesis_inhibitor', 'bacterial_dna_gyrase_inhibitor', 'bacterial_dna_inhibitor', 'bacterial_membrane_integrity_inhibitor', 'bcl_inhibitor', 'bcr-abl_inhibitor', 'benzodiazepine_receptor_agonist', 'beta_amyloid_inhibitor', 'bromodomain_inhibitor', 'btk_inhibitor', 'calcineurin_inhibitor', 'calcium_channel_blocker', 'cannabinoid_receptor_agonist', 'cannabinoid_receptor_antagonist', 'carbonic_anhydrase_inhibitor', 'casein_kinase_inhibitor', 'caspase_activator', 'catechol_o_methyltransferase_inhibitor', 'cc_chemokine_receptor_antagonist', 'cck_receptor_antagonist', 'cdk_inhibitor', 'chelating_agent', 'chk_inhibitor', 'chloride_channel_blocker', 'cholesterol_inhibitor', 'cholinergic_receptor_antagonist', 'coagulation_factor_inhibitor', 'corticosteroid_agonist', 'cyclooxygenase_inhibitor', 'cytochrome_p450_inhibitor', 'dihydrofolate_reductase_inhibitor', 'dipeptidyl_peptidase_inhibitor', 'diuretic', 'dna_alkylating_agent', 'dna_inhibitor', 'dopamine_receptor_agonist', 'dopamine_receptor_antagonist', 'egfr_inhibitor', 'elastase_inhibitor', 'erbb2_inhibitor', 'estrogen_receptor_agonist', 'estrogen_receptor_antagonist', 'faah_inhibitor', 'farnesyltransferase_inhibitor', 'fatty_acid_receptor_agonist', 'fgfr_inhibitor', 'flt3_inhibitor', 'focal_adhesion_kinase_inhibitor', 'free_radical_scavenger', 'fungal_squalene_epoxidase_inhibitor', 'gaba_receptor_agonist', 'gaba_receptor_antagonist', 'gamma_secretase_inhibitor', 'glucocorticoid_receptor_agonist', 'glutamate_inhibitor', 'glutamate_receptor_agonist', 'glutamate_receptor_antagonist', 'gonadotropin_receptor_agonist', 'gsk_inhibitor', 'hcv_inhibitor', 'hdac_inhibitor', 'histamine_receptor_agonist', 'histamine_receptor_antagonist', 'histone_lysine_demethylase_inhibitor', 'histone_lysine_methyltransferase_inhibitor', 'hiv_inhibitor', 'hmgcr_inhibitor', 'hsp_inhibitor', 'igf-1_inhibitor', 'ikk_inhibitor', 'imidazoline_receptor_agonist', 'immunosuppressant', 'insulin_secretagogue', 'insulin_sensitizer', 'integrin_inhibitor', 'jak_inhibitor', 'kit_inhibitor', 'laxative', 'leukotriene_inhibitor', 'leukotriene_receptor_antagonist', 'lipase_inhibitor', 'lipoxygenase_inhibitor', 'lxr_agonist', 'mdm_inhibitor', 'mek_inhibitor', 'membrane_integrity_inhibitor', 'mineralocorticoid_receptor_antagonist', 'monoacylglycerol_lipase_inhibitor', 'monoamine_oxidase_inhibitor', 'monopolar_spindle_1_kinase_inhibitor', 'mtor_inhibitor', 'mucolytic_agent', 'neuropeptide_receptor_antagonist', 'nfkb_inhibitor', 'nicotinic_receptor_agonist', 'nitric_oxide_donor', 'nitric_oxide_production_inhibitor', 'nitric_oxide_synthase_inhibitor', 'norepinephrine_reuptake_inhibitor', 'nrf2_activator', 'opioid_receptor_agonist', 'opioid_receptor_antagonist', 'orexin_receptor_antagonist', 'p38_mapk_inhibitor', 'p-glycoprotein_inhibitor', 'parp_inhibitor', 'pdgfr_inhibitor', 'pdk_inhibitor', 'phosphodiesterase_inhibitor', 'phospholipase_inhibitor', 'pi3k_inhibitor', 'pkc_inhibitor', 'potassium_channel_activator', 'potassium_channel_antagonist', 'ppar_receptor_agonist', 'ppar_receptor_antagonist', 'progesterone_receptor_agonist', 'progesterone_receptor_antagonist', 'prostaglandin_inhibitor', 'prostanoid_receptor_antagonist', 'proteasome_inhibitor', 'protein_kinase_inhibitor', 'protein_phosphatase_inhibitor', 'protein_synthesis_inhibitor', 'protein_tyrosine_kinase_inhibitor', 'radiopaque_medium', 'raf_inhibitor', 'ras_gtpase_inhibitor', 'retinoid_receptor_agonist', 'retinoid_receptor_antagonist', 'rho_associated_kinase_inhibitor', 'ribonucleoside_reductase_inhibitor', 'rna_polymerase_inhibitor', 'serotonin_receptor_agonist', 'serotonin_receptor_antagonist', 'serotonin_reuptake_inhibitor', 'sigma_receptor_agonist', 'sigma_receptor_antagonist', 'smoothened_receptor_antagonist', 'sodium_channel_inhibitor', 'sphingosine_receptor_agonist', 'src_inhibitor', 'steroid', 'syk_inhibitor', 'tachykinin_antagonist', 'tgf-beta_receptor_inhibitor', 'thrombin_inhibitor', 'thymidylate_synthase_inhibitor', 'tlr_agonist', 'tlr_antagonist', 'tnf_inhibitor', 'topoisomerase_inhibitor', 'transient_receptor_potential_channel_antagonist', 'tropomyosin_receptor_kinase_inhibitor', 'trpv_agonist', 'trpv_antagonist', 'tubulin_inhibitor', 'tyrosine_kinase_inhibitor', 'ubiquitin_specific_protease_inhibitor', 'vegfr_inhibitor', 'vitamin_b', 'vitamin_d_receptor_agonist', 'wnt_inhibitor']\n",
    "GENES             = [col for col in TRAIN_COLUMNS if col.startswith('g-')]\n",
    "CELLS             = [col for col in TRAIN_COLUMNS if col.startswith('c-')]\n",
    "GENE_PCA_COMP     = 450\n",
    "CELL_PCA_COMP     = 45\n",
    "VARIANCE_THRESHOLD= 0.67\n",
    "VERBOSE           = True\n",
    "FOLDS             = 7\n",
    "INPUT_SIZE        = None\n",
    "OUTPUT_SIZE       = None\n",
    "SAVE_FOLDS        = True\n",
    "USE_SAVED_FOLDS   = True\n",
    "\n",
    "PATH              = \"../data/\"\n",
    "TRAIN_F           = os.path.join(PATH, \"train_features.csv\")\n",
    "TRAIN_T           = os.path.join(PATH, \"train_targets_scored.csv\")\n",
    "TRAIN_T_NS        = os.path.join(PATH, \"train_targets_nonscored.csv\")\n",
    "TEST_F            = os.path.join(PATH, \"test_features.csv\")\n",
    "SAMPLE_SUBMISSION = os.path.join(PATH, \"sample_submission.csv\")\n",
    "PRE_FEATURES_CSV  = os.path.join(PATH, \"preprocessed_train_features.csv\")\n",
    "PRE_TARGETS_CSV   = os.path.join(PATH, \"preprocessed_train_targets.csv\")\n",
    "PRE_TEST_CSV      = os.path.join(PATH, \"preprocessed_test_features.csv\")\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        if targets is not None:\n",
    "            self.is_test = False\n",
    "        else:\n",
    "            self.is_test = True\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test:\n",
    "            return torch.tensor(self.features[index, :], dtype=torch.float)\n",
    "        else:\n",
    "            return (torch.tensor(self.features[index, :], dtype=torch.float), \n",
    "                    torch.tensor(self.targets[index, :], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADatasetModule(pl.LightningDataModule):\n",
    "    def __init__(self, fold=0):\n",
    "        super().__init__()\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.fold = fold\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        if USE_SAVED_FOLDS and os.path.isfile(PRE_FEATURES_CSV) and os.path.isfile(PRE_TARGETS_CSV) and os.path.isfile(PRE_TEST_CSV):\n",
    "            self.f = pd.read_csv(PRE_FEATURES_CSV)\n",
    "            self.t = pd.read_csv(PRE_TARGETS_CSV)\n",
    "            self.test_f = pd.read_csv(PRE_TEST_CSV)\n",
    "            \n",
    "            if VERBOSE:\n",
    "                print(\"Saved Folds Loaded.\")\n",
    "            \n",
    "        else:\n",
    "            f = pd.read_csv(TRAIN_F)\n",
    "            t = pd.read_csv(TRAIN_T)\n",
    "            t_s = pd.read_csv(TRAIN_T_NS)\n",
    "            test_f = pd.read_csv(TEST_F)\n",
    "\n",
    "            f, t, test_f = self._extract_features(f, t, test_f)\n",
    "            f, t, test_f = self._stratify(f, t, test_f, shuffle=False)\n",
    "            self.f = f\n",
    "            self.t = t\n",
    "            self.test_f = test_f\n",
    "            \n",
    "            del f, t, test_f\n",
    "            gc.collect()\n",
    "\n",
    "            if SAVE_FOLDS:\n",
    "                self.f.to_csv(PRE_FEATURES_CSV, index=False)\n",
    "                self.t.to_csv(PRE_TARGETS_CSV, index=False)\n",
    "                self.test_f.to_csv(PRE_TEST_CSV, index=False)\n",
    "                \n",
    "                if VERBOSE:\n",
    "                    print(\"Folds Saved.\")\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(\"Dataset Prepared.\")\n",
    "        \n",
    "    def _stratify(self, f, t, test_f, shuffle=True):\n",
    "        f.loc[:, \"kfold\"] = -1\n",
    "        if shuffle:\n",
    "            f = f.sample(frac=1).reset_index(drop=True)        \n",
    "        \n",
    "        mskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "        for fold_, (train_, val_) in enumerate(mskf.split(X=f, y=t)): \n",
    "            f.loc[val_, \"kfold\"] = fold_\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(\"Dataset Stratified.\")\n",
    "        return f, t, test_f\n",
    "    \n",
    "    def _extract_features(self, f, t, test_f):\n",
    "        f, t, test_f = self._mapping_and_filter(f, t, test_f)\n",
    "#         f, t, test_f = self._bin_columns(f, t, test_f)\n",
    "#         f, t, test_f = self._naive_outlier_removal(f, t, test_f)\n",
    "#         f, t, test_f = self._add_stats(f, t, test_f)\n",
    "        f, t, test_f = self._quantile_transform(f, t, test_f)\n",
    "        f, t, test_f = self._PCA(f, t, test_f, drop_original=False)\n",
    "        f, t, test_f = self._variance_thresholding(f, t, test_f)\n",
    "#         f, t, test_f = self._scaling(f, t, test_f)\n",
    "        f, t, test_f = self._align_features_and_targets(f, t, test_f)\n",
    "    \n",
    "        if VERBOSE:\n",
    "            print(\"Features Extracted.\")\n",
    "        return f, t, test_f\n",
    "    \n",
    "    def _align_features_and_targets(self, f, t, test_f):\n",
    "        columns = [\"sig_id\"]\n",
    "        columns.extend(TARGET_COLUMNS)\n",
    "        t = f.merge(t, how=\"inner\", on=\"sig_id\").loc[:, columns]\n",
    "        if VERBOSE:\n",
    "            print(\"Features and Targets Aligned.\")\n",
    "            \n",
    "        del columns\n",
    "        gc.collect()\n",
    "        \n",
    "        return f, t, test_f\n",
    "    \n",
    "    def _mapping_and_filter(self, f, t, test_f, drop_cp=True):\n",
    "        cp_type = {'trt_cp': 0, 'ctl_vehicle': 1}\n",
    "        cp_dose = {'D1': 0, 'D2': 1}\n",
    "        cp_time = {24: 0, 48: 1, 72:2}\n",
    "        for df in [f, test_f]:\n",
    "            df['cp_type'] = df['cp_type'].map(cp_type)\n",
    "            df['cp_dose'] = df['cp_dose'].map(cp_dose)\n",
    "            df['cp_time'] = df['cp_time'].map(cp_time)\n",
    "        if drop_cp:\n",
    "            t = t[f['cp_type'] == 0].reset_index(drop = True)\n",
    "            f = f[f['cp_type'] == 0].reset_index(drop = True)\n",
    "            f = f.drop(\"cp_type\", axis=1)\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(\"Features Mapped to Integers.\")\n",
    "        \n",
    "        del cp_type, cp_dose, cp_time\n",
    "        gc.collect()\n",
    "        \n",
    "        return f, t, test_f\n",
    "    \n",
    "    def _scaling(self, f, t, test_f):\n",
    "        features = f.columns[3:]\n",
    "        scaler = RobustScaler()\n",
    "        scaler.fit(pd.concat([f[features], test_f[features]], axis = 0))\n",
    "        f[features] = scaler.transform(f[features])\n",
    "        test_f[features] = scaler.transform(test_f[features])\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(\"Features Scaled.\")\n",
    "            \n",
    "        del features, scaler\n",
    "        gc.collect()\n",
    "        \n",
    "        return f, t, test_f\n",
    "    \n",
    "    def _variance_thresholding(self, f, t, test_f):\n",
    "        var_thresh = VarianceThreshold(threshold=VARIANCE_THRESHOLD)\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(f\"Before Variance Thresholding {f.shape[1]} features present.\")\n",
    "        \n",
    "        data = f.append(test_f)\n",
    "        var_thresh.fit(data.iloc[:, 3:])\n",
    "        data_transformed = data.iloc[:, 3:][data.iloc[:, 3:].columns[var_thresh.get_support(indices=True)]]\n",
    "        train_features_transformed = data_transformed.iloc[:f.shape[0]]\n",
    "        test_features_transformed = data_transformed.iloc[-test_f.shape[0]:]\n",
    "\n",
    "        train_features = pd.DataFrame(f[['sig_id', 'cp_time', 'cp_dose']].values.reshape(-1, 3),\n",
    "                                      columns=['sig_id', 'cp_time',  'cp_dose'])\n",
    "        f = pd.concat([train_features, train_features_transformed], axis=1)\n",
    "        test_features = pd.DataFrame(test_f[['sig_id', 'cp_time', 'cp_dose']].values.reshape(-1, 3),\n",
    "                                     columns=['sig_id', 'cp_time', 'cp_dose'])\n",
    "        test_f = pd.concat([test_features, test_features_transformed], axis=1)\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(f\"After Variance Thresholding {f.shape[1]} features present.\")\n",
    "            print(\"Variance Thresholding Done.\")\n",
    "\n",
    "        del data, data_transformed, train_features_transformed, test_features_transformed, train_features, test_features\n",
    "        gc.collect()\n",
    "        \n",
    "        return f, t, test_f\n",
    "        \n",
    "    def _quantile_transform(self, f, t, test_f):\n",
    "        QUANTILE_COLUMNS = [c for c in f.columns if c.startswith('c-')] + [c for c in f.columns if c.startswith('g-')] \n",
    "#         + [c for c in f.columns if c.startswith('stat-')]\n",
    "        for col in (QUANTILE_COLUMNS):\n",
    "            transformer = QuantileTransformer(n_quantiles=100, random_state=SEED, output_distribution=\"normal\")\n",
    "            vec_len = len(f[col].values)\n",
    "            vec_len_test = len(test_f[col].values)\n",
    "            raw_vec = f[col].values.reshape(vec_len, 1)\n",
    "            transformer.fit(raw_vec)\n",
    "\n",
    "            f[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "            test_f[col] = transformer.transform(test_f[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(\"Features Quantile Transformed.\")\n",
    "            \n",
    "        del transformer, vec_len, vec_len_test, raw_vec, QUANTILE_COLUMNS\n",
    "        gc.collect()\n",
    "        \n",
    "        return f, t, test_f\n",
    "    \n",
    "    def _add_stats(self, f, t, test_f):    \n",
    "        for df in [f, test_f]:\n",
    "            df['stat-g_sum'] = df[GENES].sum(axis = 1)\n",
    "            df['stat-g_mean'] = df[GENES].mean(axis = 1)\n",
    "            df['stat-g_std'] = df[GENES].std(axis = 1)\n",
    "            df['stat-g_kurt'] = df[GENES].kurtosis(axis = 1)\n",
    "            df['stat-g_skew'] = df[GENES].skew(axis = 1)\n",
    "            df['stat-c_sum'] = df[CELLS].sum(axis = 1)\n",
    "            df['stat-c_mean'] = df[CELLS].mean(axis = 1)\n",
    "            df['stat-c_std'] = df[CELLS].std(axis = 1)\n",
    "            df['stat-c_kurt'] = df[CELLS].kurtosis(axis = 1)\n",
    "            df['stat-c_skew'] = df[CELLS].skew(axis = 1)\n",
    "            df['stat-gc_sum'] = df[GENES + CELLS].sum(axis = 1)\n",
    "            df['stat-gc_mean'] = df[GENES + CELLS].mean(axis = 1)\n",
    "            df['stat-gc_std'] = df[GENES + CELLS].std(axis = 1)\n",
    "            df['stat-gc_kurt'] = df[GENES + CELLS].kurtosis(axis = 1)\n",
    "            df['stat-gc_skew'] = df[GENES + CELLS].skew(axis = 1)\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(\"Feature Stats Added.\")\n",
    "        \n",
    "        return f, t, test_f\n",
    "        \n",
    "        \n",
    "    def _bin_columns(self, f, t, test_f, drop_original=False):\n",
    "        for col in GENES:\n",
    "            f.loc[:, f'{col}_bin'] = pd.cut(f[col], bins=3, labels=False)\n",
    "            test_f.loc[:, f'{col}_bin'] = pd.cut(test_f[col], bins=3, labels=False)\n",
    "\n",
    "        if drop_original:\n",
    "            f.drop(GENES).reset_index(drop=True)\n",
    "            test_f.drop(GENES).reset_index(drop=True)\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(\"Features Binned.\")\n",
    "            \n",
    "        return f, t, test_f\n",
    "        \n",
    "    def _naive_outlier_removal(self, f, t, test_f):\n",
    "        train_ = f.copy() # Didn't wanted to actually normalize, so created a copy and normalized that for further calculation\n",
    "        for col in GENES:\n",
    "        #     train_[col] = (train[col]-np.mean(train[col])) / (np.std(train[col]))\n",
    "            mean = train_[col].mean()\n",
    "            std = train_[col].std()\n",
    "\n",
    "            std_r = mean + 4*std\n",
    "            std_l = mean - 4*std\n",
    "\n",
    "            drop = train_[col][(train_[col]>std_r) | (train_[col]<std_l)].index.values\n",
    "\n",
    "        f = f.drop(drop).reset_index(drop=True)\n",
    "        t = t.drop(drop).reset_index(drop=True)\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(\"Feature Outliers Removed.\")\n",
    "            \n",
    "        del train_, mean, std, std_r, std_l, drop\n",
    "        gc.collect()\n",
    "            \n",
    "        return f, t, test_f\n",
    "        \n",
    "    def _PCA(self, f, t, test_f, drop_original=False):\n",
    "        def create_pca(train, test, features, kind, n_components):\n",
    "            train_ = train[features].copy()\n",
    "            test_ = test[features].copy()\n",
    "            data = pd.concat([train_, test_], axis = 0)\n",
    "            pca = PCA(n_components = n_components,  random_state = SEED)\n",
    "            data = pca.fit_transform(data)\n",
    "            columns = [f'{kind}-pca-{i + 1}' for i in range(n_components)]\n",
    "            data = pd.DataFrame(data, columns = columns)\n",
    "            train_ = data.iloc[:train.shape[0]]\n",
    "            test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n",
    "            train = pd.concat([train, train_], axis = 1)\n",
    "            test = pd.concat([test, test_], axis = 1)\n",
    "            return train, test\n",
    "\n",
    "        f, test_f = create_pca(f, test_f, GENES, kind = 'g', n_components = GENE_PCA_COMP)\n",
    "        f, test_f = create_pca(f, test_f, CELLS, kind = 'c', n_components = CELL_PCA_COMP)\n",
    "        if drop_original:\n",
    "            f = f.drop(GENES).reset_index(drop=True)\n",
    "            t = t.drop(CELLS).reset_index(drop=True)\n",
    "                \n",
    "        if VERBOSE:\n",
    "            print(\"PCA Performed.\")            \n",
    "            \n",
    "        return f, t, test_f\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        global INPUT_SIZE, OUTPUT_SIZE\n",
    "        train_X = self.f[self.f.kfold != self.fold]\n",
    "        self.train_X = train_X.drop(['kfold', 'sig_id'], axis=1).to_numpy().astype('float64')\n",
    "        train_Y = self.t[self.f.kfold != self.fold]\n",
    "        self.train_Y = train_Y.drop(['sig_id'], axis=1).to_numpy().astype('float64')\n",
    "        \n",
    "        INPUT_SIZE = self.train_X.shape[1]\n",
    "        OUTPUT_SIZE = self.train_Y.shape[1]\n",
    "        \n",
    "        print(f\"Dataset has {INPUT_SIZE} features.\")\n",
    "        \n",
    "        valid_X = self.f[self.f.kfold == self.fold]\n",
    "        self.valid_X = valid_X.drop(['kfold', 'sig_id'], axis=1).to_numpy().astype('float64')\n",
    "        valid_Y = self.t[self.f.kfold == self.fold]\n",
    "        self.valid_Y = valid_Y.drop(['sig_id'], axis=1).to_numpy().astype('float64')\n",
    "        \n",
    "        self.test_X = self.test_f.to_numpy()\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print(\"Dataset Setup.\")\n",
    "            \n",
    "        del train_X, train_Y, valid_X, valid_Y\n",
    "        gc.collect()\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        if VERBOSE:\n",
    "            print(\"Train Dataloader Loaded.\")\n",
    "        return DataLoader(MoADataset(self.train_X, self.train_Y), self.batch_size, num_workers=0, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        if VERBOSE:\n",
    "            print(\"Validation Dataloader Loaded.\")\n",
    "        return DataLoader(MoADataset(self.valid_X, self.valid_Y), self.batch_size, num_workers=0, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        if VERBOSE:\n",
    "            print(\"Test Dataloader Loaded.\")\n",
    "        return DataLoader(MoaDataset(self.test_X, None), self.batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to update INPUT_SIZE and OUTPUT_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0, pos_weight=None):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        if pos_weight is not None:\n",
    "            self.pos_weight = torch.tensor(pos_weight).cuda()\n",
    "        else:\n",
    "            self.pos_weight = pos_weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                                           self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight, pos_weight=self.pos_weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoANet1(nn.Module):\n",
    "    name = \"MoANet1\"\n",
    "    def __init__(self, num_features, num_targets, trial=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.LAYER_OUTPUTS = [2048, 4096, 2048, 1024, 512]\n",
    "        self.DROPOUT = 0.5\n",
    "        self.NUM_LAYERS = 5\n",
    "        \n",
    "        if trial is not None:\n",
    "            f_dropout = trial.suggest_float('f_dropout', 0.2, 0.5)\n",
    "        else:\n",
    "            f_dropout = self.DROPOUT\n",
    "        \n",
    "        layers = []\n",
    "\n",
    "        # Intermediate layers\n",
    "        in_size = num_features\n",
    "        for i in range(self.NUM_LAYERS):\n",
    "            out_size = self.LAYER_OUTPUTS[i]\n",
    "#             out_size = trial.suggest_int('n_units_{}'.format(i), 256, 4096)\n",
    "            layers.append(nn.utils.weight_norm(torch.nn.Linear(in_size, self.LAYER_OUTPUTS[i], bias=False), name=f'weight'))\n",
    "            layers.append(nn.BatchNorm1d(out_size))\n",
    "            layers.append(nn.Dropout(f_dropout))\n",
    "            layers.append(nn.PReLU())\n",
    "            in_size = out_size\n",
    "\n",
    "        # Final layer\n",
    "        layers.append(torch.nn.Linear(in_size, num_targets))\n",
    "    \n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.model.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias != None:\n",
    "                m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoANet2(nn.Module):\n",
    "    name = \"MoANet2\"\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(MoANet2, self).__init__()\n",
    "        \n",
    "        self.HIDDEN_LAYER_SIZE = 1500\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.HIDDEN_LAYER_SIZE))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(self.HIDDEN_LAYER_SIZE)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(self.HIDDEN_LAYER_SIZE, self.HIDDEN_LAYER_SIZE))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(self.HIDDEN_LAYER_SIZE)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(self.HIDDEN_LAYER_SIZE, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoANet3(nn.Module):\n",
    "    name = \"MoANet3\"\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(MoANet3, self).__init__()\n",
    "        \n",
    "        self.HIDDEN_LAYER_SIZE = 1500\n",
    "        \n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.HIDDEN_LAYER_SIZE))\n",
    "        self.batch_norm1 = nn.BatchNorm1d(self.HIDDEN_LAYER_SIZE)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(self.HIDDEN_LAYER_SIZE, self.HIDDEN_LAYER_SIZE))\n",
    "        self.batch_norm2 = nn.BatchNorm1d(self.HIDDEN_LAYER_SIZE)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(self.HIDDEN_LAYER_SIZE, num_targets))\n",
    "        self.batch_norm3 = nn.BatchNorm1d(num_targets)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoANet4(nn.Module):\n",
    "    name = \"MoANet4\"\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(MoANet4, self).__init__()\n",
    "        \n",
    "        self.HIDDEN_LAYER_SIZE_1 = 1536\n",
    "        self.HIDDEN_LAYER_SIZE_2 = 2048\n",
    "        self.HIDDEN_LAYER_SIZE_3 = 1536\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.HIDDEN_LAYER_SIZE_1))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(self.HIDDEN_LAYER_SIZE_1)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(self.HIDDEN_LAYER_SIZE_1, self.HIDDEN_LAYER_SIZE_2))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(self.HIDDEN_LAYER_SIZE_2)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(self.HIDDEN_LAYER_SIZE_2, self.HIDDEN_LAYER_SIZE_3))\n",
    "        \n",
    "        self.batch_norm4 = nn.BatchNorm1d(self.HIDDEN_LAYER_SIZE_3)\n",
    "        self.dropout4 = nn.Dropout(0.25)\n",
    "        self.dense4 = nn.utils.weight_norm(nn.Linear(self.HIDDEN_LAYER_SIZE_3, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.leaky_relu(self.dense3(x))\n",
    "        \n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.dense4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    name = \"LogisticRegression\"\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "                \n",
    "        self.batch_norm = nn.BatchNorm1d(num_features)\n",
    "        self.dense = nn.utils.weight_norm(nn.Linear(num_features, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepLearning(nn.Module):\n",
    "    name = \"WideAndDeepLearning\"\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(WideAndDeepLearning, self).__init__()\n",
    "        \n",
    "        self.HIDDEN_LAYER_SIZE = 2048\n",
    "        \n",
    "        hidden_size = self.HIDDEN_LAYER_SIZE\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.wide   = nn.utils.weight_norm(nn.Linear(num_features, num_targets))\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "        \n",
    "        self.batch_norm4 = nn.BatchNorm1d(2 * num_targets)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dense4 = nn.utils.weight_norm(nn.Linear(2 * num_targets, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x2 = F.leaky_relu(self.dense1(x))\n",
    "        w = F.leaky_relu(self.wide(x))\n",
    "        \n",
    "        x = self.batch_norm2(x2)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        x = w + x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReluBnDropout(nn.Module):\n",
    "    name = \"LinearReluBnDropout\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearReluBnDropout, self).__init__()\n",
    "        \n",
    "        self.DROPOUT = 0.4\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(in_features, out_features)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.Dropout(self.DROPOUT)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TablarNet(nn.Module):\n",
    "    name = \"TablarNet\"\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(TablarNet, self).__init__()\n",
    "        \n",
    "        self.EMBEDDING_DIMENSIONS = [(2, 15), (3, 20), (2, 15)]\n",
    "        self.DROPOUT = 0.4\n",
    "        self.HIDDEN_SIZE = 2048\n",
    "        self.CONTINUOUS_FEATURES = num_features - 3\n",
    "\n",
    "        self.embedding_layer = nn.ModuleList([nn.Embedding(x, y) for x, y in self.EMBEDDING_DIMENSIONS])\n",
    "        self.dropout = nn.Dropout(self.DROPOUT, inplace=True)\n",
    "\n",
    "        self.first_bn_layer = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.CONTINUOUS_FEATURES),\n",
    "            nn.Dropout(self.DROPOUT)\n",
    "        )\n",
    "\n",
    "        first_in_feature = self.CONTINUOUS_FEATURES + sum([y for x, y in self.EMBEDDING_DIMENSIONS])\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            LinearReluBnDropout(in_features=first_in_feature,\n",
    "                                out_features=self.HIDDEN_SIZE),\n",
    "            LinearReluBnDropout(in_features=self.HIDDEN_SIZE,\n",
    "                                out_features=self.HIDDEN_SIZE)\n",
    "        )\n",
    "\n",
    "        self.last = nn.Linear(self.HIDDEN_SIZE, num_targets)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cont_f = x[:, 3:]\n",
    "        cat_f = x[:, :3]\n",
    "\n",
    "        cat_x = [layer(cat_f[:, i].long()) for i, layer in enumerate(self.embedding_layer)]\n",
    "        cat_x = torch.cat(tuple(cat_x), 1)\n",
    "        cat_x = self.dropout(cat_x)\n",
    "\n",
    "        cont_x = self.first_bn_layer(cont_f)\n",
    "\n",
    "        x = torch.cat([cont_x, cat_x], 1)\n",
    "\n",
    "        x = self.block(x)\n",
    "        x = self.last(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLoss(y_true, y_pred):\n",
    "    score = 0\n",
    "    for i in range(y_true.shape[1]):\n",
    "        x = y_true[:, i]\n",
    "        z = y_pred[:, i]\n",
    "        score_ = log_loss(x, z, labels=[0, 1])\n",
    "        score += score_ / y_true.shape[1]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "class MoALitModule(pl.LightningModule):\n",
    "    def __init__(self, net, fold):\n",
    "        super(MoALitModule, self).__init__()\n",
    "        self.valid_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.net = net\n",
    "        self.fold = fold\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        self.sklearn_valid_losses = []\n",
    "        self.epoch = 1\n",
    "        self.best_valid_loss = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             params=self.parameters(), \n",
    "#             lr=LEARNING_RATE,\n",
    "#             weight_decay=WEIGHT_DECAY\n",
    "#         )\n",
    "#         scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#             optimizer,\n",
    "#             patience=0,\n",
    "#             factor=SCHEDULER_FACTOR,\n",
    "#             verbose=LEARNING_VERBOSE\n",
    "#         )\n",
    "#         return {\n",
    "#            'optimizer': optimizer,\n",
    "#            'lr_scheduler': scheduler,\n",
    "#            'monitor': 'val_loss'\n",
    "#        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=0)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):        \n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        \n",
    "        if WEIGHT is not 1:\n",
    "            self.train_criterion = SmoothBCEwLogits(smoothing=0.001, weight=((targets * (WEIGHT - 1)) + 1))\n",
    "        else:\n",
    "            self.train_criterion = SmoothBCEwLogits(smoothing=0.001)\n",
    "        \n",
    "        loss = self.train_criterion(outputs, targets)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        losses = 0.0\n",
    "        for output in outputs:\n",
    "            loss = output[\"loss\"]\n",
    "            losses += loss.item()\n",
    "            \n",
    "        losses /= len(outputs)\n",
    "        \n",
    "        print(f\"Training Epoch {self.epoch} Loss: {losses}\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.valid_criterion(outputs, targets)\n",
    "        sklearn_loss = logLoss(y_true=targets.detach().cpu().numpy().astype('float64'),\n",
    "                               y_pred=torch.sigmoid(outputs).detach().cpu().numpy().astype('float64'))\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('sklearn_loss', sklearn_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        self.valid_losses.append(loss.item())\n",
    "        self.sklearn_valid_losses.append(sklearn_loss.item())\n",
    "        \n",
    "        return loss, sklearn_loss\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        losses = 0.0\n",
    "        sklearn_losses = 0.0\n",
    "        for output in outputs:\n",
    "            loss, sklearn_loss = output\n",
    "            losses += loss.item()\n",
    "            sklearn_losses += sklearn_loss\n",
    "            \n",
    "        print(f\"Average Validation Loss for Fold {self.fold}: {sum(self.valid_losses) / len(self.valid_losses)}\")\n",
    "            \n",
    "        losses /= len(outputs)\n",
    "        sklearn_losses /= len(outputs)\n",
    "         \n",
    "        if self.best_valid_loss is not None:\n",
    "            self.best_valid_loss = min(self.best_valid_loss, losses)\n",
    "        else:\n",
    "            self.best_valid_loss = losses\n",
    "        \n",
    "        print(f\"Validation Epoch         {self.epoch} Loss: {losses}\")\n",
    "        print(f\"SKLearn Validation Epoch {self.epoch} Loss: {sklearn_losses}\")\n",
    "        self.epoch += 1\n",
    "        \n",
    "    def teardown(self, stage):\n",
    "        print(f\"Average Training Loss for Fold           {self.fold}: {sum(self.train_losses) / len(self.train_losses)}\")\n",
    "        print(f\"Average Validation Loss for Fold         {self.fold}: {sum(self.valid_losses) / len(self.valid_losses)}\")\n",
    "        print(f\"Average SKLearn Validation Loss for Fold {self.fold}: {sum(self.sklearn_valid_losses) / len(self.sklearn_valid_losses)}\")\n",
    "        print(f\"Best Validation Loss for Fold            {self.fold}: {self.best_valid_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_VERBOSE  = False\n",
    "BATCH_SIZE        = 128\n",
    "EARLY_STOPPING    = 10\n",
    "LEARNING_RATE     = 0.01\n",
    "WEIGHT            = 1\n",
    "WEIGHT_DECAY      = 2e-5\n",
    "SCHEDULER_FACTOR  = 0.5\n",
    "SAVE_TOP_K        = 1\n",
    "MAX_EPOCHS        = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Callback\n",
    "class MetricsCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "        self.lr = LEARNING_RATE\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "#         print(trainer.lr_schedulers)\n",
    "#         print(trainer.optimizers)\n",
    "        for scheduler in trainer.lr_schedulers:\n",
    "            param_groups = scheduler['scheduler'].optimizer.param_groups\n",
    "            lr = param_groups[0][\"lr\"]\n",
    "            if lr < self.lr:\n",
    "                print(f\"Learning Rate Reduced to {lr}\")\n",
    "            if lr > self.lr:\n",
    "                print(f\"Learning Rate Increased to {lr}\")\n",
    "                \n",
    "            self.lr = lr\n",
    "#             print(f\"Learning Rate: {lr}\")\n",
    "        self.metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(net, fold):\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(\n",
    "        save_dir=\"tb_logs\", \n",
    "        name=f\"base_fold_{fold}\", \n",
    "        version=0\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(\n",
    "        logging_interval='step'\n",
    "    )\n",
    "\n",
    "    metrics_callback = MetricsCallback()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=f'weights/{net.name}/moa-net={net.name}-fold={fold}-' + '{epoch:03d}-{val_loss:.5f}',\n",
    "        monitor='val_loss',\n",
    "        save_top_k=SAVE_TOP_K,\n",
    "        verbose=LEARNING_VERBOSE\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.00,\n",
    "        mode='min',\n",
    "        patience=EARLY_STOPPING,\n",
    "        verbose=LEARNING_VERBOSE\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger=tb_logger,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        gpus=1, # -1 if torch.cuda.is_available() else None,\n",
    "        callbacks=[lr_monitor, metrics_callback, early_stop_callback],\n",
    "        checkpoint_callback=checkpoint_callback, # Do not save any checkpoints,\n",
    "    )\n",
    "    \n",
    "    return trainer, checkpoint_callback, metrics_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(net, fold, prediction_ids, submission_ids, prediction, submission, metrics_and_models):\n",
    "    print(f\"Running on Fold #{fold}\")\n",
    "    trainer, checkpoint_callback, metrics_callback = get_trainer(net, fold)\n",
    "\n",
    "    dataset = MoADatasetModule(fold=fold)\n",
    "    dataset.prepare_data()\n",
    "    dataset.setup()\n",
    "\n",
    "    prediction_ids += dataset.f[dataset.f.kfold==fold][\"sig_id\"].to_list()\n",
    "    submission_ids = dataset.test_f[\"sig_id\"].to_list()\n",
    "\n",
    "    model = MoALitModule(net(INPUT_SIZE, OUTPUT_SIZE), fold)\n",
    "    trainer.fit(model, dataset)\n",
    "\n",
    "    print(f\"Best Model for Fold #{fold} saved at {checkpoint_callback.best_model_path}.\")\n",
    "    model = MoALitModule.load_from_checkpoint(checkpoint_callback.best_model_path, net=net(INPUT_SIZE, OUTPUT_SIZE), fold=fold)\n",
    "\n",
    "    prediction = np.concatenate([\n",
    "        prediction,\n",
    "        model(torch.tensor(dataset.valid_X.astype(\"float32\"))).sigmoid().detach().cpu().float().numpy()\n",
    "    ], axis=0)\n",
    "\n",
    "    if submission is not None:\n",
    "        submission = submission + (model(torch.tensor(dataset.test_X[:, 1:].astype(\"float32\"))).sigmoid().detach().cpu().float().numpy() / FOLDS)\n",
    "    else:\n",
    "        submission = model(torch.tensor(dataset.test_X[:, 1:].astype(\"float32\"))).sigmoid().detach().cpu().float().numpy() / FOLDS\n",
    "\n",
    "    best_model_score = logLoss(dataset.valid_Y.astype('float64'),\n",
    "                               model(torch.tensor(dataset.valid_X.astype(\"float32\"))).sigmoid().detach().cpu().float().numpy().astype('float64'))\n",
    "    \n",
    "    metrics_and_models[net.name][\"fold_metrics\"].append(best_model_score)\n",
    "    metrics_and_models[net.name][\"fold_monitor\"].append(metrics_callback)\n",
    "    metrics_and_models[net.name][\"fold_models_path\"].append(checkpoint_callback.best_model_path)\n",
    "    \n",
    "    hello_world()\n",
    "\n",
    "    del tb_logger, lr_monitor, metrics_callback, checkpoint_callback, early_stop_callback, dataset, trainer, model\n",
    "    gc.collect()\n",
    "    \n",
    "    return prediction, submission, prediction_ids, submission_ids, metrics_and_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model : MoANet2\n",
      "Running on Fold #0\n",
      "Saved Folds Loaded.\n",
      "Dataset Prepared.\n",
      "Dataset has 1092 features.\n",
      "Dataset Setup.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | valid_criterion | BCEWithLogitsLoss | 0     \n",
      "1 | net             | MoANet2           | 4 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dataloader Loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.6967620551586151\n",
      "Validation Epoch         1 Loss: 0.6967620551586151\n",
      "SKLearn Validation Epoch 1 Loss: 0.6967620481357617\n",
      "Train Dataloader Loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d5f040e2d74672b59d97410ca519c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.07034793454739782\n",
      "Validation Epoch         2 Loss: 0.020234804898500443\n",
      "SKLearn Validation Epoch 2 Loss: 0.020234804724473116\n",
      "Training Epoch 3 Loss: 0.0841736388171003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.045664051798387215\n",
      "Validation Epoch         3 Loss: 0.01900545842945576\n",
      "SKLearn Validation Epoch 3 Loss: 0.01900545860636831\n",
      "Learning Rate Reduced to 0.009890738003669028\n",
      "Training Epoch 4 Loss: 0.022848630070584974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.03670043947005814\n",
      "Validation Epoch         4 Loss: 0.018056125827133656\n",
      "SKLearn Validation Epoch 4 Loss: 0.018056125842320837\n",
      "Learning Rate Reduced to 0.009567727288213004\n",
      "Training Epoch 5 Loss: 0.021661042267469323\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.031983919825185746\n",
      "Validation Epoch         5 Loss: 0.017457039318978785\n",
      "SKLearn Validation Epoch 5 Loss: 0.01745703926148191\n",
      "Learning Rate Reduced to 0.009045084971874737\n",
      "Training Epoch 6 Loss: 0.020674473320951268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.029057764822221176\n",
      "Validation Epoch         6 Loss: 0.01711905241012573\n",
      "SKLearn Validation Epoch 6 Loss: 0.017119052413890383\n",
      "Learning Rate Reduced to 0.008345653031794291\n",
      "Training Epoch 7 Loss: 0.019716191441327535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.02702179070758192\n",
      "Validation Epoch         7 Loss: 0.0166790422052145\n",
      "SKLearn Validation Epoch 7 Loss: 0.016679041849447313\n",
      "Learning Rate Reduced to 0.0075\n",
      "Training Epoch 8 Loss: 0.018817411102101105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.025560638409549908\n",
      "Validation Epoch         8 Loss: 0.01667683243751526\n",
      "SKLearn Validation Epoch 8 Loss: 0.016676832266223324\n",
      "Learning Rate Reduced to 0.006545084971874737\n",
      "Training Epoch 9 Loss: 0.017633889896833167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.024479331102886116\n",
      "Validation Epoch         9 Loss: 0.016823675371706485\n",
      "SKLearn Validation Epoch 9 Loss: 0.016823675265866495\n",
      "Learning Rate Reduced to 0.005522642316338268\n",
      "Training Epoch 10 Loss: 0.016228030828543665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.023660864032772143\n",
      "Validation Epoch         10 Loss: 0.01704765010625124\n",
      "SKLearn Validation Epoch 10 Loss: 0.017047650152906563\n",
      "Learning Rate Reduced to 0.004477357683661734\n",
      "Training Epoch 11 Loss: 0.014562191986510542\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.023055128073171963\n",
      "Validation Epoch         11 Loss: 0.017555045560002328\n",
      "SKLearn Validation Epoch 11 Loss: 0.017555045388779854\n",
      "Learning Rate Reduced to 0.003454915028125264\n",
      "Training Epoch 12 Loss: 0.01248985511839998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.02258995004563125\n",
      "Validation Epoch         12 Loss: 0.01790095552802086\n",
      "SKLearn Validation Epoch 12 Loss: 0.01790095572890346\n",
      "Learning Rate Reduced to 0.0025000000000000014\n",
      "Training Epoch 13 Loss: 0.010421988123482993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.02224330022462354\n",
      "Validation Epoch         13 Loss: 0.018402420207858084\n",
      "SKLearn Validation Epoch 13 Loss: 0.01840242027986453\n",
      "Learning Rate Reduced to 0.0016543469682057106\n",
      "Training Epoch 14 Loss: 0.008728968720807105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.021969345942230763\n",
      "Validation Epoch         14 Loss: 0.018659978210926055\n",
      "SKLearn Validation Epoch 14 Loss: 0.01865997817987105\n",
      "Learning Rate Reduced to 0.0009549150281252633\n",
      "Training Epoch 15 Loss: 0.007642239563445858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.02174785719464787\n",
      "Validation Epoch         15 Loss: 0.01885078437626362\n",
      "SKLearn Validation Epoch 15 Loss: 0.018850784445394047\n",
      "Learning Rate Reduced to 0.0004322727117869951\n",
      "Training Epoch 16 Loss: 0.007090769318521631\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.021561938523258075\n",
      "Validation Epoch         16 Loss: 0.01894420363008976\n",
      "SKLearn Validation Epoch 16 Loss: 0.018944203548227866\n",
      "Learning Rate Reduced to 0.00010926199633097157\n",
      "Training Epoch 17 Loss: 0.006842715820918481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.021398267263907995\n",
      "Validation Epoch         17 Loss: 0.018930104672908784\n",
      "SKLearn Validation Epoch 17 Loss: 0.01893010484008929\n",
      "Learning Rate Reduced to 0.0\n",
      "Training Epoch 18 Loss: 0.006757365207054785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss for Fold 0: 0.021255713920228934\n",
      "Validation Epoch         18 Loss: 0.01896345615386963\n",
      "SKLearn Validation Epoch 18 Loss: 0.018963456256933353\n",
      "Learning Rate Increased to 0.00010926199633097155\n",
      "Training Epoch 19 Loss: 0.006766599929276981\n",
      "\n",
      "Average Training Loss for Fold           0: 0.017826823619607628\n",
      "Average Validation Loss for Fold         0: 0.021255713920228934\n",
      "Average SKLearn Validation Loss for Fold 0: 0.021255713870134797\n",
      "Best Validation Loss for Fold            0: 0.01667683243751526\n",
      "Best Model for Fold #0 saved at D:\\Kevin\\Machine Learning\\MoA Prediction\\notebooks\\weights\\MoANet2\\moa-net=MoANet2-fold=0-epoch=006-val_loss=0.01422.ckpt.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hello_world' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2c3d1732fc1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmission_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_and_models\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_fold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmission_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_and_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-94294afe013f>\u001b[0m in \u001b[0;36mrun_fold\u001b[1;34m(net, fold, prediction_ids, submission_ids, prediction, submission, metrics_and_models)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mmetrics_and_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fold_models_path\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mhello_world\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mtb_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_monitor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hello_world' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "nets = [MoANet2]\n",
    "folds = list(range(FOLDS))[:1]\n",
    "# nets = [LogisticRegression]\n",
    "metrics_and_models = {}\n",
    "predictions = []\n",
    "submissions = []\n",
    "\n",
    "def display_validation_loss():\n",
    "    avg_validation_loss = 0.0\n",
    "    for fold in folds:\n",
    "        valid_loss = metrics_and_models[net.name][\"fold_metrics\"][fold]\n",
    "        print(f\"Validation Loss for Fold {fold}: {valid_loss}\")\n",
    "        avg_validation_loss += valid_loss\n",
    "\n",
    "    avg_validation_loss /= FOLDS\n",
    "    print(f\"Average Validation Loss: {avg_validation_loss}\")\n",
    "\n",
    "for net in nets:\n",
    "    print(f\"Training Model : {net.name}\")\n",
    "    PREDICTION = os.path.join(PATH, f\"{net.name}-prediction.csv\")\n",
    "    SUBMISSION = os.path.join(PATH, f\"{net.name}-submission.csv\")\n",
    "    prediction_ids = []\n",
    "    submission_ids = []\n",
    "    columns = [\"sig_id\"]\n",
    "    columns.extend(TARGET_COLUMNS)\n",
    "    \n",
    "    prediction = np.empty((0, len(columns) - 1))\n",
    "    submission = None\n",
    "    \n",
    "    metrics_and_models[net.name] = {}\n",
    "    metrics_and_models[net.name][\"fold_metrics\"] = []\n",
    "    metrics_and_models[net.name][\"fold_models_path\"]  = []\n",
    "    metrics_and_models[net.name][\"fold_monitor\"] = []\n",
    "\n",
    "    for fold in folds:\n",
    "        prediction, submission, prediction_ids, submission_ids, metrics_and_models = run_fold(net, fold, prediction_ids, submission_ids, prediction, submission, metrics_and_models)\n",
    "\n",
    "    prediction = pd.DataFrame(np.concatenate([np.array(prediction_ids).reshape(-1, 1), prediction], axis=1), columns=columns)\n",
    "    prediction.to_csv(PREDICTION, index=False)\n",
    "    submission = pd.DataFrame(np.concatenate([np.array(submission_ids).reshape(-1, 1), submission], axis=1), columns=columns)\n",
    "    submission.to_csv(SUBMISSION, index=False)\n",
    "    \n",
    "    display_validation_loss()\n",
    "        \n",
    "    predictions.append(prediction)\n",
    "    submissions.append(submission)\n",
    "    del prediction, submission\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_and_models['MoANet2']['fold_metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_validation_loss = 0.0\n",
    "for fold in folds:\n",
    "    print(f\"Validation Loss for Fold {fold}: {fold_metrics[fold]}\")\n",
    "    avg_validation_loss += fold_metrics[fold]\n",
    "\n",
    "avg_validation_loss /= FOLDS\n",
    "print(f\"Average Validation Loss: {avg_validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple : 0.022132062166929246\n",
    "# PCA + RobustScaler : 0.022057733684778213\n",
    "# PCA + RobustScaler + Variance Thresholding : 0.02237522266805172\n",
    "# Quantile + PCA + Variance Thresholding + MoANet2 : 0.02158106155693531\n",
    "# Quantile + PCA + Variance Thresholding + MoANet2 : 0.02375772302704198\n",
    "# No Shuffle + TablarNet + PCA : CV = 0.015230209566652775 | LB = 0.01926\n",
    "# No Shuffle + Logistic Regression + PCA : CV = 0.014702482149004936 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = np.zeros((test_f.shape[0], len(TARGET_COLUMNS)))\n",
    "\n",
    "# for fold in folds:\n",
    "#     predictions += (fold_models[fold](torch.tensor(test_f[:, 1:].astype(\"float\"))).sigmoid().detach().cpu().numpy() / FOLDS)\n",
    "    \n",
    "#     fold_metrics.append(metrics_callback.metrics)\n",
    "#     fold_models.append(model)\n",
    "    \n",
    "# columns = [\"sig_id\"]\n",
    "# columns.extend(TARGET_COLUMNS)\n",
    "# predictions = pd.DataFrame(np.concatenate((test_f[:, 0].reshape(-1, 1), predictions), axis=1), columns=columns)\n",
    "# predictions.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "b = np.array([1, 2, 3, 4, 5, 6]).reshape(2, 3)\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions = pd.read_csv(\"submission.csv\")\n",
    "# submissions.iloc[:, 1:] = submissions.iloc[:, 1:].astype(\"float\")\n",
    "# submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submissions = pd.read_csv(SAMPLE_SUBMISSION)\n",
    "# sample_submissions.columns == submissions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submissions.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(4, 5)\n",
    "b = np.random.randn(4, 5)\n",
    "b += a / FOLDS\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: https://www.kaggle.com/bootiu/moa-pytorch-lightning-baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
